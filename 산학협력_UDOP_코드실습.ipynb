{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwWiQ37iRXWGnctwWKi3t8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naye971012/2023_sanhak_ML_study/blob/main/%EC%82%B0%ED%95%99%ED%98%91%EB%A0%A5_UDOP_%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#moudel import"
      ],
      "metadata": {
        "id": "9ihu7kRa1kR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "SpZi2Z7W1nMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OjcnIPHxRvQR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from timm.models.vision_transformer import PatchEmbed, DropPath"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from typing import Any, Dict, Optional, Sequence, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "\n",
        "from transformers import T5Config, T5PreTrainedModel\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "from transformers.models.t5.modeling_t5 import T5Block, T5ForConditionalGeneration, T5LayerNorm"
      ],
      "metadata": {
        "id": "oz706r304Y0U"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#params"
      ],
      "metadata": {
        "id": "-f46DPaS1lzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IN_CHAN=3 #input chanel of image, 3 since it is RGB\n",
        "BATCH_SIZE=128 #batch size\n",
        "IMG_SIZE=224 * 224 #image size\n",
        "PATCH_SIZE=16 #patch size\n",
        "TXT_SIZE = 12 #    # of text tokens\n",
        "EMB_SIZE = 128 #embedding size\n",
        "NUM_CLASS=10\n",
        "\n",
        "\n",
        "\n",
        "VOCAB_SIZE = TXT_SIZE+1 #used when text embedding\n",
        "MLP_LAYER = int(IMG_SIZE/pow(PATCH_SIZE,2)) + 1 + TXT_SIZE #classification decoder에 사용"
      ],
      "metadata": {
        "id": "Hv2hDsse_7kD"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load data (구현 필요)"
      ],
      "metadata": {
        "id": "h2KuLOFRzYKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#embedding (현재 1D positional embedding, 추후 교체 필요)"
      ],
      "metadata": {
        "id": "X-TFLfd_JIHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class image_EmbeddingLayer(nn.Module):\n",
        "    def __init__(self,in_chan, img_size, patch_size, batch_size=128):\n",
        "        super().__init__()\n",
        "        self.num_patches = int(img_size / pow(patch_size, 2)) # total number of patches\n",
        "        self.emb_size = EMB_SIZE\n",
        "\n",
        "        self.project = nn.Conv2d(in_chan, self.emb_size, kernel_size= patch_size, stride=patch_size) # image embedding\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,self.emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn(self.num_patches, self.emb_size)) # 1D positional embedding\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project(x)\n",
        "        x = x.view(-1, self.num_patches , self.emb_size ) # [batch_size, # of patches, embedding size]\n",
        "        #repeat_cls = self.cls_token.repeat(x.size()[0],1,1) #[ batch_size, 1 , 1]\n",
        "        #x = torch.cat((repeat_cls, x), dim=1) #[batch_size, # of patches + 1, embedding_size]\n",
        "        x += self.positions\n",
        "        return x # [batch size, # of patches + 1, embedding size]\n",
        "\n",
        "class text_EmbeddingLayer(nn.Module):\n",
        "  def __init__(self,txt_size,emb_size,vocab_size):\n",
        "    super().__init__()\n",
        "    self.txt_size = txt_size\n",
        "    self.emb_size = emb_size\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.project = nn.Embedding(num_embeddings=vocab_size,embedding_dim=self.emb_size) #num_embeddings = 단어 idx최대값 + 1\n",
        "    self.cls_token = nn.Parameter(torch.randn(1,1,self.emb_size))\n",
        "    self.positions = nn.Parameter(torch.randn(self.txt_size+ 1, self.emb_size)) # 1D positional embedding\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.project(x) #[ batch size, # of token ] -> [batch size, # of token, embedding size]\n",
        "    x = x.view(-1,self.txt_size,self.emb_size) # [batch_size, txt_size, embedding size]\n",
        "    repeat_cls = self.cls_token.repeat(x.size()[0],1,1) #[batch_size, 1 , 1]\n",
        "    x = torch.cat((repeat_cls,x),dim=1)\n",
        "    x += self.positions\n",
        "    return x"
      ],
      "metadata": {
        "id": "mdBCXgkQ8qHv"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## this block is for testing ###############\n",
        "image_embedding = image_EmbeddingLayer(in_chan=IN_CHAN, img_size=IMG_SIZE, patch_size=PATCH_SIZE, batch_size = BATCH_SIZE)\n",
        "text_embedding = text_EmbeddingLayer(txt_size=TXT_SIZE, emb_size=EMB_SIZE, vocab_size =VOCAB_SIZE)\n",
        "\n",
        "image_test = torch.randn(1,3,224,224) #[batch size, # of channel, image size x, image size y] -> [batch size, # of patches, embedding size]\n",
        "text_test = torch.randint(0, 11, (1, TXT_SIZE)) #[batch size, # of token] -> [batch size, # of token, embedding size]\n",
        "\n",
        "\n",
        "image_output = image_embedding(image_test)\n",
        "text_output = text_embedding(text_test)\n",
        "print(image_output.shape, text_output.shape) # torch.Size([1, 197, 128]) torch.Size([1, 13, 128])\n",
        "\n",
        "embedding_output = torch.cat((text_output,image_output,),dim=1) # [batch_size, # of total tokens, # of embedding layer]\n",
        "print(embedding_output.shape) # torch.Size([1, 209, 128])\n",
        "######### testing ends ###############################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrTf7V74DNWO",
        "outputId": "84bbbc91-d893-465c-cedd-5b7f510a6360"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 196, 128]) torch.Size([1, 13, 128])\n",
            "torch.Size([1, 209, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#encoder"
      ],
      "metadata": {
        "id": "8UTMedhcJJUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "t_VAaMWK17Q3"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "OzqgIGI406jW"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, N, C = x.shape\n",
        "        _, N_context, _ = context.shape\n",
        "\n",
        "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        k = self.k(context).reshape(B, N_context, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        v = self.v(context).reshape(B, N_context, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cB0WoEbE1WZj"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_cross_attention=False):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "\n",
        "        self.use_cross_attention = use_cross_attention\n",
        "        if use_cross_attention:\n",
        "            self.cross_attn = CrossAttention(\n",
        "                dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "            self.norm_ct = norm_layer(dim)\n",
        "\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        if context is not None and self.use_cross_attention:\n",
        "            x = x + self.drop_path(self.cross_attn(self.norm_ct(x), context))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "kZbF7wfX1Z2V"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_cross_attention=False):\n",
        "    super().__init__()\n",
        "    self.block = Block(dim,num_heads, drop=drop,attn_drop=attn_drop,drop_path=drop_path)\n",
        "  def forward(self,x):\n",
        "    return self.block(x)"
      ],
      "metadata": {
        "id": "IFKqN4dOJKpg"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################this block is for testing ######################\n",
        "\n",
        "encoder = Encoder(dim=EMB_SIZE, num_heads=8, drop=0., attn_drop=0., drop_path=0.)\n",
        "\n",
        "#print(embedding_output.shape) # torch.Size([1, 209, 128]) #\n",
        "\n",
        "encoder_output = encoder(embedding_output)\n",
        "\n",
        "print(encoder_output.shape)\n",
        "\n",
        "######################### testing end ##################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKRZTNK3TK8k",
        "outputId": "9cd84ca0-45db-4d57-e49e-f9101edc1149"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 209, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#decoder"
      ],
      "metadata": {
        "id": "3qUN1CVw1IAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_classification(nn.Module):\n",
        "  def __init__(self,in_feature,hidden_feature,output_feature,drop):\n",
        "    super().__init__()\n",
        "    self.output = Mlp(in_features=in_feature * MLP_LAYER, hidden_features=hidden_feature,out_features=output_feature, drop=0.) #final layer이므로 drop x\n",
        "  def forward(self,x):\n",
        "    x = x.view(-1,EMB_SIZE * MLP_LAYER)\n",
        "    return self.output(x)"
      ],
      "metadata": {
        "id": "HxGudu6RztvV"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_nlp(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "rd7_qdFW_Nrt"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,in_feature,hidden_feature,output_feature,drop=0.):\n",
        "    super().__init__()\n",
        "    self.classification_decoder = Decoder_classification(in_feature,hidden_feature,output_feature,drop)\n",
        "    self.nlp_decoder = Decoder_nlp()\n",
        "\n",
        "  def forward(self,x,is_classification_decoder):\n",
        "    if(is_classification_decoder):\n",
        "      return self.classification_decoder(x)\n",
        "    else:\n",
        "      return self.nlp_decoder(x)"
      ],
      "metadata": {
        "id": "OMicpr-o1JMv"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################this block is for testing ######################\n",
        "\n",
        "decoder = Decoder(in_feature=EMB_SIZE, hidden_feature=EMB_SIZE, output_feature=NUM_CLASS, drop=0.)\n",
        "\n",
        "#print(encoder_output.shape) # torch.Size([1, 209, 128])\n",
        "\n",
        "decoder_output = decoder(encoder_output,is_classification_decoder=True)\n",
        "\n",
        "print(decoder_output.shape)\n",
        "\n",
        "######################### testing end ##################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-GUY1Jh_iUi",
        "outputId": "a601a903-bd99-4338-efc2-59d534853db9"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model summary"
      ],
      "metadata": {
        "id": "uZtk_2j81JfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class model_summary(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.image_embedding = image_EmbeddingLayer(in_chan=IN_CHAN, img_size=IMG_SIZE, patch_size=PATCH_SIZE, batch_size = BATCH_SIZE)\n",
        "    self.text_embedding = text_EmbeddingLayer(txt_size=TXT_SIZE, emb_size=EMB_SIZE, vocab_size =VOCAB_SIZE)\n",
        "    self.encoder = Encoder(dim=EMB_SIZE, num_heads=8, drop=0., attn_drop=0., drop_path=0.)\n",
        "    self.decoder = Decoder(in_feature=EMB_SIZE, hidden_feature=EMB_SIZE, output_feature=NUM_CLASS, drop=0.)\n",
        "\n",
        "  def embedding(self,image,text):\n",
        "    image_output = self.image_embedding(image)\n",
        "    text_output = self.text_embedding(text)\n",
        "    embedding_output = torch.cat((text_output,image_output,),dim=1) # [batch_size, # of total tokens, # of embedding layer]\n",
        "    return embedding_output\n",
        "\n",
        "  def forward(self,x):\n",
        "    encoder_output = self.encoder(x)\n",
        "    decoder_output = self.decoder(x,is_classification_decoder=True)\n",
        "    return decoder_output"
      ],
      "metadata": {
        "id": "M2tLbLYX1Lr-"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_summary()\n",
        "\n",
        "image_test = torch.randn(1,3,224,224) #[batch size, # of channel, image size x, image size y] -> [batch size, # of patches, embedding size]\n",
        "text_test = torch.randint(0, 11, (1, TXT_SIZE)) #[batch size, # of token] -> [batch size, # of token, embedding size]\n",
        "\n",
        "embedding_test = model.embedding(image_test,text_test)\n",
        "output = model(embedding_test)\n",
        "\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnJ_NVr0Q-y3",
        "outputId": "f172ce32-be4d-41d8-997e-79e3c078079a"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    }
  ]
}